#### 从本机复制到集群

```  
sudo scp -i /home/qly/.ssh/11899517 hadoop-2.7.6.tar.gz  11899517@10.173.32.6:/mnt/home/11899517/
```  

#### 解压

```  
tar zxf hadoop-2.7.6.tar.gz
```  

#### 创建软链接指向某某文件夹

```  
ln -s hadoop-2.7.6 hadoop-current
```  

##### 配置文件路径：/mnt/home/11899517/hadoop-current/etc/hadoop

#### 概念注意：

```  
namenode管理文件系统的命名空间，维护着文件系统树及整棵树内所有的文件和目录
记录着每个文件中各个块所在的数据节点信息，但它并不永久保存块的位置信息，
因为这些信息会在系统启动时根据数据节点信息重建。
```  

#### vi hdfs-site.xml
<li>dfs.namenode.name.dir：
    为了保证这两种元数据文件的高可用性，一般的做法，将dfs.namenode.name.dir设置成以逗号分隔的多个目录，
    这多个目录至少不要在一块磁盘上，最好放在不同的机器上
<li>dfs.datanode.data.dir 存放DN数据的本地文件夹路径

```  
<configuration>
<property>
<name>dfs.namenode.rpc-address</name>
<value>bigdata0:33601</value>
</property>
<property>
<name>dfs.namenode.http-address</name>
<value>bigdata0: 33602</value>
</property>
<property>
<name>dfs.datanode.address</name>
<value>bigdata3:33603</value>
</property>
<property>
<name>dfs.datanode.http.address</name>
<value>bigdata3:33604</value>
</property>
<property>
<name>dfs.datanode.ipc.address</name>
<value>bigdata3:33605</value>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>/mnt/home/11899517/name</value>
</property>
<property>
<name>dfs.datanode.data.dir</name>
<value>/mnt/home/11899517/data/0,/mnt/home/11899517/data/1,/mnt/home/11899517/data/2</value>
</property>
</configuration>
``` 

#### vi core-site.xml
<li>fs.defaultFS：默认文件系统，namenode运行地址
<li>fs.replication：文件系统复本数
    
``` 
<configuration>
<property>
<name>fs.defaultFS</name>
 <value>hdfs://bigdata0:33601</value>
</property>
</configuration>
```                











