
<li>pom

```
package com.bigdata.etl.job;

import com.bigdata.etl.mr.LogFieldWritable;
import com.bigdata.etl.mr.LogGenericWritable;
import com.bigdata.etl.mr.LogOutputFormat;
import com.bigdata.etl.mr.TextLongGroupComparator;
import com.bigdata.etl.mr.TextLongPartitioner;
import com.bigdata.etl.mr.TextLongWritable;
import com.bigdata.etl.utils.IPUtil;

import java.io.IOException;
import java.net.URI;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Counter;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONArray;
import com.alibaba.fastjson.JSONObject;

/**
 * sessionId排序
 * <li>MR程序课堂代码
 * @author qly
 *
 */
public class ParseLogJob extends Configured implements Tool{
	
	/**
	 * 解析日志
	 * https://gitlab.com/yktceshi/testdata/blob/master/IPUtil.java
	 * @param row
	 * @return
	 * @throws ParseException 
	 */
	public static LogGenericWritable parseLog(String row) throws Exception{
		String[] logPart = row.split("\u1111");
		SimpleDateFormat dateFormat =new SimpleDateFormat("yyy-MM-dd HH:mm:ss.SSS");
		long timeTag=dateFormat.parse(logPart[0]).getTime();
		String activeName=logPart[1];
		JSONObject bizObj=JSON.parseObject(logPart[2]);
		
		LogGenericWritable log=new LogWritable();
		log.put("time_tag",new LogFieldWritable(timeTag));
		log.put("active_name", new LogFieldWritable(activeName));
		//key与getFieldName的相同，则直接put即可
		for(Map.Entry<String, Object> entry:bizObj.entrySet()){
			log.put(entry.getKey(), new LogFieldWritable(entry.getValue()));
		}
/*		LogBeanWritable log=new LogBeanWritable();
		log.setActiveName(activeName);
		log.setTimeTag(timeTag);
		log.setDeviceID(bizObj.getString("device_id"));
		log.setIp(bizObj.getString("ip"));
		log.setOrderID(bizObj.getString("order_id"));
		log.setProductID(bizObj.getString("product_id"));
		log.setRegUrl(bizObj.getString("reg_url"));
		log.setSessionID(bizObj.getString("session_id"));
		log.setUserID(bizObj.getString("user_id"));*/

		return log;
	}
	
	public static class LogWritable extends LogGenericWritable{
		@Override
		protected String[] getFieldName() {
			return new String[] {"active_name","session_id","time_tag","ip","device_id","req_url","user_id","product_id","order_id","error_flag","error_log"};
		}	
	}
	
	/**
	 * 
	 * @author qly
	 */
	public static class LogMapper extends Mapper<LongWritable,Text,TextLongWritable,LogGenericWritable>{
		/**
		v * 重写hadoop的map方法，解析日志
		 * <li>context-上下文变量
		 */
		public void map(LongWritable key,Text value,Context context) throws IOException, InterruptedException{
			//添加计数器记录错误value数目
			Counter errorCounter = context.getCounter("Log_error","Parse_error");
			
			try {
				LogGenericWritable parseLog = parseLog(value.toString());
				//根据业务定义输出的key值-原来是sessionId，现在是userId
				String session=(String)parseLog.getObject("session_id");
				Long timeTag=(Long)parseLog.getObject("time_tag");
				TextLongWritable ok=new TextLongWritable();
				ok.setText(new Text(session));
				ok.setCompareValue(new LongWritable(timeTag));
				
				context.write(ok, parseLog);
				//总的记录数 计时器
				//context.getCounter("Log_normal","Parse_map_user_input").increment(1);
			} catch (Exception e) {//Exception超类能捕捉所有异常
				errorCounter.increment(1);
				//e.printStackTrace();
				//在解析日志方法中LogGenericWritable，可能会发生数组越界报错，若发生报错，则交由reduce端处理
				LogGenericWritable v=new LogWritable();
				v.put("error_flag", new LogFieldWritable("error"));
				v.put("error_log", new LogFieldWritable(value));
				TextLongWritable outKey=new TextLongWritable();
				int randomKey=(int)(Math.random()*100);
				outKey.setText(new Text("error"+randomKey));
				context.write(outKey, v);
			}		
		}
	}
	
	/**
	 * 
	 * @author qly
	 *
	 */
	public static class LogReducer extends Reducer<TextLongWritable,LogGenericWritable,Text,Text>{
		//定义sessionId-实质是userId
		private Text sessionId;
		//定义一个完整的访问路径
		private JSONArray actionPath=new JSONArray();	
		
		/**
		 * 读取配置文件
		 * @throws IOException 
		 */
		public void setup(Context context) throws IOException{
			//采用分布式缓存后，不需要代码去读取客户端的配置文件
/*			Configuration conf=context.getConfiguration();
			FileSystem fs=FileSystem.get(conf);
			Path ipFile=new Path(conf.get("ip.file.path"));
			Path ipFile=new Path("/user/hadoop/lib/17monipdb.dat");
			Path localPath=new Path(this.getClass().getResource("/").getPath());
			fs.copyToLocalFile(ipFile, localPath);*/
			IPUtil.load("17monipdb.dat");
		}
		
		/**
		 * reduce在while中执行
		 */
		public void reduce(TextLongWritable key,Iterable<LogGenericWritable> values,Context context)  throws IOException, InterruptedException {
			//按userId分类后需依据userId分别读取value
			Text sid=key.getText();
			if(sessionId == null || !sid.equals(sessionId)){
				//context.getCounter("Log_normal","reduce_sessionId_change").increment(1);
				//sessionId发生变更
				sessionId=new Text(sid);
				actionPath.clear();//确保两个session的用户路径不会放到同一个Path里面
			}	
			
			//同一个session的访问路径未按session分类，则只需循环读取每个value即可
			for(LogGenericWritable v:values){		
				JSONObject datum=JSON.parseObject(v.asJsonString());
				if(v.getObject("error_flag")==null){
					//context.getCounter("Log_normal","normal_reduce_data").increment(1);
					//获取IP的省市区
					String ip=(String)v.getObject("ip");
					String[] address=IPUtil.find(ip);
					JSONObject obj=new JSONObject();
					obj.put("country",address[0]);
					obj.put("province",address[1]);
					obj.put("city",address[2]);
					datum.put("address", obj);
					
					//获取sctieName和requestUrl
					String activeName=(String)v.getObject("active_name");
					String reqUrl=(String)v.getObject("req_url");
					String pathUnit="pageview".equals(activeName) ? reqUrl : activeName;
					actionPath.add(pathUnit);
					
					datum.put("action_path", actionPath);
				}
				String outputKey=v.getObject("error_flag")==null? "part":"error/part";
				context.write(new Text(outputKey), new Text(datum.toJSONString()));
			}
		}
		
		
	}
	
	/**
	 * 继承Configured实现run
	 */
	public int run(String[] args) throws Exception{
		//读取配置文件
		Configuration config=getConf();
		config.addResource("mr.xml");
		Job job = Job.getInstance(config);
		job.setJarByClass(ParseLogJob.class);
		job.setJobName("parseLog");
		job.setMapperClass(LogMapper.class);
		//版本3新增
		job.setReducerClass(LogReducer.class);
		job.setMapOutputKeyClass(TextLongWritable.class);
		job.setMapOutputValueClass(LogWritable.class);
		job.setOutputValueClass(Text.class);
		
		//增加设置分类和比较排序的参数
		job.setPartitionerClass(TextLongPartitioner.class);
		job.setGroupingComparatorClass(TextLongGroupComparator.class);
		
		//自定义输入输出类型
		job.setInputFormatClass(CombineTextInputFormat.class);
		job.setOutputFormatClass(LogOutputFormat.class);
		
		//使用分布式缓存
		job.addCacheFile(new URI(config.get("ip.file.path")));
		//job.addCacheFile(new URI(new Path(args[3]));
		
		FileInputFormat.addInputPath(job, new Path(args[0]));
		Path outputPath=new Path(args[1]);
		FileOutputFormat.setOutputPath(job, outputPath);
		//LZO
//		FileOutputFormat.setCompressOutput(job, true);
//		FileOutputFormat.setCompressOutput(job, LzopCoder.class);
		
		FileSystem fs=FileSystem.get(config);
		if(!fs.exists(outputPath)){
			fs.delete(outputPath,true);
		}
		if(!job.waitForCompletion(true)){
			throw new RuntimeException(job.getJobName()+"failed!");
		}
		return 0;
	}
	
	public static void main(String[] args) throws Exception {
		int res=ToolRunner.run(new Configuration(), new ParseLogJob(),args);
		System.exit(res);
	}

}

```


<li>pom

```
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" 
		xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  		xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <groupId>netease.bigdata.course</groupId>
  <artifactId>had1205</artifactId>
  <version>0.0.1-SNAPSHOT</version>

  <dependencies>

    <dependency>
  		<groupId>org.apache.hadoop</groupId>
  		<artifactId>hadoop-client</artifactId>
  		<version>2.7.6</version>
  		<scope>provided</scope>
  	</dependency>
  	<dependency>
  		<groupId>com.alibaba</groupId>
  		<artifactId>fastjson</artifactId>
  		<version>1.2.4</version>
  	</dependency>
<!--   	<dependency>
  		<groupId>org.anarres.lzo</groupId>
  		<artifactId>lzo-hadoop</artifactId>
  		<version>1.0.0</version>
  	</dependency> -->
  </dependencies>

  <build>
      <plugins>
        <plugin>
  			<groupId>org.apache.maven.plugins</groupId>
  			<artifactId>maven-assembly-plugin</artifactId>
  			<configuration>
  				<descriptorRefs>
  					<descriptorRef>
  						jar-with-dependencies
  					</descriptorRef>
  				</descriptorRefs>
  				<source>1.8</source>
            	<target>1.8</target>
  			</configuration>
  			<executions>
  				<execution>
  					<id>make-assembly</id>
  					<phase>package</phase>
  					<goals>
  						<goal>single</goal>
  					</goals>
  				</execution>
  			</executions>
  		</plugin>
      </plugins>
  </build>
      <!--使用国内阿里的maven镜像地址-->
    <repositories>
        <repository>
            <id>nexus-aliyun</id>
            <name>nexus-aliyun</name>
            <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
            <releases>
                <enabled>true</enabled>
            </releases>
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </repository>
    </repositories>
</project>
```

<li>src/main/resources/mr.xml

```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<property>
		<name>ip.file.path</name>
		<!-- 本机 -->
		<value>hdfs://localhost:9000/user/hResource/17monipdb.dat</value>
		<!-- 本人集群 -->
<!-- 		<value>hdfs://bigdata0:33601/user/11899517/resource/17monipdb.dat</value> -->
		<!-- 老师集群 -->
<!-- 		<value>hdfs://bigdata0:50000/user/hadoop/lib/17monipdb.dat</value> -->
	</property>
</configuration>

```
