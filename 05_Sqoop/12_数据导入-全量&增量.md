### 全量数据导入

全量数据导入就是一次性将所有需要导入的数据，从关系型数据库一次性地导入到Hadoop中（可以是HDFS、Hive等）。
全量导入形式使用场景为一次性离线分析场景。
用sqoop import命令，具体如下：

```
# 全量数据导入 sqoop import \ --connect jdbc:mysql://192.168.xxx.xxx:3316/testdb \
--username root \
--password 123456 \
--query “select * from test_table where \$CONDITIONS” \
--target-dir /user/root/person_all \ 
--fields-terminated-by “,” \
--hive-drop-import-delims \
--null-string “\\N” \
--null-non-string “\\N” \
--split-by id \
-m 6 \
```

```
– query 	SQL查询语句
– target-dir 	HDFS目标目录（确保目录不存在，否则会报错，因为Sqoop在导入数据至HDFS时会自己在HDFS上创建目录）
–hive-drop-import- delims 	删除数据中包含的Hive默认分隔符（^A, ^B, \n）
–null-string 	string类型空值的替换符（Hive中Null用\n表示）
–null-non-string 	非string类型空值的替换符
–split-by 	数据切片字段（int类型，m>1时必须指定）
-m 	Mapper任务数，默认为4
```

### 增量数据导入

在生产环境中，系统可能会定期从与业务相关的关系型数据库向Hadoop导入数据，导入数仓后进行后续离线分析。
增量数据导入分两种，一是基于递增列的增量数据导入（Append方式）。二是基于时间列的增量数据导入（LastModified方式）

<li>Append方式

```
举个栗子，有一个订单表，里面每个订单有一个唯一标识自增列ID，在关系型数据库中以主键形式存在。
之前已经将id在0~5201314之间的编号的订单导入到Hadoop中了（这里为HDFS），
现在一段时间后我们需要将近期产生的新的订单数据导入Hadoop中（这里为HDFS），以供后续数仓进行分析。
此时我们只需要指定–incremental 参数为append，–last-value参数为5201314即可。表示只从id大于5201314后开始导入。
```

```
# Append方式的全量数据导入 sqoop import \ --connect jdbc:mysql://192.168.xxx.xxx:3316/testdb \
--username root \
--password 123456 \
--query “select order_id, name from order_table where \$CONDITIONS” \
--target-dir /user/root/orders_all \ 
--split-by order_id \
-m 6  \
--incremental append \
--check-column order_id \
--last-value 5201314
```

```
–incremental append 	基于递增列的增量导入（将递增列值大于阈值的所有数据增量导入Hadoop）
–check-column 	递增列（int）
–last-value 	阈值（int）
```

<li>lastModify方式

```
此方式要求原有表中有time字段，它能指定一个时间戳，让Sqoop把该时间戳之后的数据导入至Hadoop（这里为HDFS）。
因为后续订单可能状态会变化，变化后time字段时间戳也会变化，此时Sqoop依然会将相同状态更改后的订单导入HDFS，
当然我们可以指定merge-key参数为orser_id，表示将后续新的记录与原有记录合并。 
```

```
# 将时间列大于等于阈值的数据增量导入HDFS sqoop import \ --connect jdbc:mysql://192.168.xxx.xxx:3316/testdb \
--username root \
--password transwarp \
--query “select order_id, name from order_table where \$CONDITIONS” \
--target-dir /user/root/order_all \ 
--split-by id \
-m 4  \
--incremental lastmodified \
--merge-key order_id \
--check-column time \
# remember this date !!! --last-value “2014-11-09 21:00:00” 
```

```
–incremental lastmodified 	基于时间列的增量导入（将时间列大于等于阈值的所有数据增量导入Hadoop）
–check-column 	时间列（int）
–last-value 	阈值（int）
–merge-key 	合并列（主键，合并键值相同的记录）
```

#### 并发导入参数

```
过 -m 参数能够设置导入数据的 map 任务数量，即指定了 -m 即表示导入方式为并发导入，
这时我们必须同时指定 - -split-by 参数指定根据哪一列来实现哈希分片，从而将不同分片的数据分发到不同 map 任务上去跑，避免数据倾斜。
```

```
生产环境中，为了防止主库被Sqoop抽崩，我们一般从备库中抽取数据。
一般RDBMS的导出速度控制在60~80MB/s，每个 map 任务的处理速度5~10MB/s 估算，即 -m 参数一般设置4~8，表示启动 4~8 个map 任务并发抽取。
```
